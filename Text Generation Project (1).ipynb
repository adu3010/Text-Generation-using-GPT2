{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2670690d-8d86-4cd8-9611-f0d9ec43e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad658b0-0f2c-4bce-9518-5cd3bef31b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  #try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for more complex versions\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=200, temperature=1.0, top_p=0.95, top_k=50):\n",
    "    # Encode the prompt text and convert it to input IDs\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text using GPT-2 model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,  # Controls randomness temperature: Controls the randomness of the generation. Lower values make the output more deterministic (e.g., temperature=0.7), while higher values increase randomness (e.g., temperature=1.5).\n",
    "            top_p=top_p,              # Nucleus sampling top_p (nucleus sampling): Controls the cumulative probability for the top-p choices. It helps make the model generate more diverse output.\n",
    "            top_k=top_k,              # Controls diversity of top-k candidates top_k: Limits the sampling pool to the top-k most likely words.\n",
    "            do_sample=True,           # Sampling rather than greedy decoding \n",
    "            pad_token_id=tokenizer.eos_token_id,  # Padding to EOS token\n",
    "            num_return_sequences=1    # Number of sequences to return\n",
    "        )\n",
    "    \n",
    "    # Decode the output IDs to text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d6c00d-37f2-42f4-a374-5aab47b609f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d62f25-e82b-4814-b0c4-b7abd10b5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I will find you a book on love and intimacy before you ask me to write it.\"\n",
      "\n",
      "\"Thank you.\" The two girls were in a good mood and seemed to have recovered.\n",
      "\n",
      "\"So, what are you going to do in your sleep?\"\n",
      "\n",
      "\"My bed has a mattress to it, but if you come out of the room with me you can just see the room, too. I want to see the world, so I should be able to see everything in a single glance.\"\n",
      "\n",
      "\"Do you know how to do that?\" the girl asked.\n",
      "\n",
      "\"No. I'm just going to say something like that, because that's not what you're looking for when you're sleeping.\"\n",
      "\n",
      "The girl had a small smile on her face, but it was so cold that the light from her head was like melting ice. It felt like a fire, like a hot fire that would melt even in winter.\n",
      "\n",
      "\"I can do that in my bed if I want to and it won't be bad. But it doesn't matter what you think. You've got to do it in your bed.\"\n",
      "\n",
      "\"So you've got to be really good at that too?\"\n",
      "\n",
      "The girl shook her head.\n",
      "\n",
      "\"I don't know.\" She said, \"I just want to see the world, and I really don't want to wait around for someone to die in my bed. I am not going to live for another\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I will find you\"\n",
    "generated_story = generate_text(prompt, max_length=300, temperature=0.9)\n",
    "\n",
    "print(f\"Generated Story:\\n{generated_story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1f1321-572b-4e87-8baa-285e1432c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "Once upon a time in a land far away, the sun and moon shone on the night and on the day they shone on the sky. But there are many days to be watched, and so many days to be watched that they do not leave away for you any time from now. Now you are a man who has lost his father's love. You are not in prison. No, no, you are not in prison. The only thing you can do is to tell the world how you feel. (Luke 11:31-40) If there is any man in the world who still feels the need of the Church for his father's love he should tell the Church about it, and they will see that there is an inner and greater need than ever before in any man of this world.\n",
      "\n",
      "He who believes in God and the Father shall never be destroyed.\n",
      "\n",
      "In the end, there are many times in this world when we are in a state of joy, but we are in a state of darkness, and we are in a state of doubt. We are in the midst of all kinds of suffering. We are in an old age in which we have lost all hope. We are in a place of sadness. We are in a state of sadness where God does not exist, where He is suffering, where He has no God, where His body is made of a stone, where He is a ghost, and so on. (Matt. 2:12-15)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Once upon a time in a land far away\"\n",
    "generated_story = generate_text(prompt, max_length=300, temperature=0.9)\n",
    "\n",
    "print(f\"Generated Story:\\n{generated_story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ab49e2-6b9e-4006-a0b8-55d54554a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "We are destined to meet.\n",
      "\n",
      "We hope that the present circumstances will set you on a path of good fortune, that you will live well and prosper.\n",
      "\n",
      "I have lived my life in this country, and my country, and my country, and my country.\n",
      "\n",
      "You may find it difficult for you to find a way out, because you do not know what is at stake, the real or the potential.\n",
      "\n",
      "But you must understand how this can all work.\n",
      "\n",
      "You may also find it easy to understand that there is something really important that you, a stranger, must do.\n",
      "\n",
      "If there is nothing you do for others, what is there for you?\n",
      "\n",
      "How long have you been alone?\n",
      "\n",
      "Why, I have thought about it that way, but I cannot say what I know about it.\n",
      "\n",
      "There is only one thing I must do, and that is to make my own life as pleasant as possible for all of my friends.\n",
      "\n",
      "I am going to write this letter now.\n",
      "\n",
      "Because this is where I will stay for the long run, I am not asking you to make a decision.\n",
      "\n",
      "I am asking you to let go your fears and begin to work.\n",
      "\n",
      "Because I'm afraid that if you do not do what I ask, you will be afraid of everything.\n",
      "\n",
      "In my country, there is no hope of making the world the way I want it to be.\n",
      "\n",
      "My country is the worst place in the world because of what I have done in it.\n",
      "\n",
      "There are people who are going to hate you, but I am telling you that one day you will see those people.\n",
      "\n",
      "It is not you who would be happy to be in this country.\n",
      "\n",
      "It is not you who would like to live in a society where you could feel safe and secure, where you could have a chance to live a better life.\n",
      "\n",
      "If you cannot live in a\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"We are destined to meet\"\n",
    "generated_story = generate_text(prompt, max_length=400, temperature=0.9)\n",
    "\n",
    "print(f\"Generated Story:\\n{generated_story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2350fce-b92e-4e33-84a7-84ee23f5056c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
