# Text-Generation-using-GPT2

# Project Overview:
The project involved the development of a Text Generation System using the pre-trained GPT-2 model from Hugging Face's Transformers library. The goal of the project was to create a system capable of generating creative text such as short stories or poems based on a given input prompt.

# Objective:
The primary objective was to build a simple and efficient text generation model that could take a user’s input (e.g., a short prompt like "Once upon a time...") and generate coherent, contextually relevant, and creative text. The model was designed to work interactively, where a user can provide a prompt, and the model generates a continuation of that prompt

# •	Libraries Used:
o	Hugging Face Transformers: This library provided access to pre-trained models, including GPT-2, and tools for easy implementation of text generation tasks.
o	PyTorch: PyTorch was used as the deep learning framework for running the model, as it is compatible with Hugging Face Transformers and offers efficient model training and inference.

